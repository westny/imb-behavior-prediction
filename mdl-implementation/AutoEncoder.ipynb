{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eedf61-197d-4305-ba5e-17977ff888b7",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "**Author:** [westny](https://github.com/westny) <br>\n",
    "**Date created:** 2021-05-04 <br>\n",
    "**Last modified:** 2021-05-04 <br>\n",
    "**Description:** Implementation Autoencoders using different strategies, including a base version (used in paper), a BNAE and Sparse AE. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sized-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-runner",
   "metadata": {},
   "source": [
    "# Encoder modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c32234-bf4e-4200-a58d-74ff2886b56a",
   "metadata": {},
   "source": [
    "### Base autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    def __init__(self, n_features=36, hidden_size=20, n_layers=1, dropout_prob=0.35):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_features, hidden_size, n_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 4:].clone()\n",
    "        hc = self.init_hidden(x.size(0))\n",
    "        _, (hidden, cell) = self.lstm(x, hc)\n",
    "        hidden = hidden[-1,:,:]\n",
    "        hidden = self.fc(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f615d1-b67b-487f-a982-e555467f80f3",
   "metadata": {},
   "source": [
    "### Autoencoder with batchnorm layer after output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderBN(nn.Module):\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    def __init__(self, n_features=36, hidden_size=20, n_layers=1, dropout_prob=0.35):\n",
    "        super(AutoEncoderBN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_features, hidden_size, n_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 4:].clone()\n",
    "        hc = self.init_hidden(x.size(0))\n",
    "        _, (hidden, cell) = self.lstm(x, hc)\n",
    "        hidden = hidden[-1,:,:]\n",
    "        hidden = self.fc(hidden)\n",
    "        hidden = self.bn(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aff0b8-8f1a-4ecc-a766-54351259ebe7",
   "metadata": {},
   "source": [
    "### Sparse encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd593c4-2742-4dcc-89ee-4760dcb2b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderSparse(nn.Module):\n",
    "    \"\"\" Should be used together with appropriate regularization, \n",
    "        e.g L1 or KL on the output.\"\"\"\n",
    "    def __init__(self, n_features=36, hidden_size=20, n_layers=1, dropout_prob=0.35):\n",
    "        super(AutoEncoderSparse, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_features, hidden_size, n_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 4:].clone()\n",
    "        hc = self.init_hidden(x.size(0))\n",
    "        _, (hidden, cell) = self.lstm(x, hc)\n",
    "        hidden = hidden[-1,:,:]\n",
    "        hidden = self.fc(hidden)\n",
    "        return F.relu(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38bda2-e8a8-429d-90ac-286571571e0c",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    def __init__(self, n_features=20, output_size=36, n_layers=1, seq_len=20, dropout_prob=0.25):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = 2 * n_features\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_features,\n",
    "                            self.hidden_size,\n",
    "                            n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device='cuda'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "        hc = self.init_hidden(x.size(0))\n",
    "        \n",
    "        x, (hidden_state, cell_state) = self.lstm(x, hc)\n",
    "\n",
    "        x = x.reshape((-1, self.seq_len, self.hidden_size))\n",
    "       \n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2378f18-2577-4d92-9aa4-8e1027266718",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderDecoder(nn.Module):\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super(AutoEncoderDecoder, self).__init__()\n",
    "        self.embedding = embedding_size\n",
    "        self.encoder = AutoEncoder(hidden_size=self.embedding)\n",
    "        self.decoder = Decoder(n_features=self.embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
