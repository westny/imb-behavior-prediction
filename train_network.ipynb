{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date created:** 2021-05-19 <br>\n",
    "**Last modified:** 2021-05-19 <br>\n",
    "**Description:** Main function to train networks using the principles described in the paper <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from random import Random\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Options:\n",
    "    epochs: int =  100 # number of epochs\n",
    "    batch_size: int = 128 # batch size\n",
    "    epoch_limit: int = 25 # number of allowed consectutive bad epochs before aborting\n",
    "    clip: float = 0.25 # gradient clipping limit\n",
    "    seed: int = 99 # seed for pseduo-random utility\n",
    "    ae_lr: float = 1e-2 # learning rate for autoencoder\n",
    "    lr: float = 1e-4 # learning rate for classifiers\n",
    "    sparse_func: str = 'none' # sparsity function: L1 | KL | none\n",
    "    reg_param: float = 2e-4 # controlled amount of sparsity\n",
    "    use_cuda: bool = True # user preference\n",
    "    device: torch.device = torch.device(\"cuda\") # default device\n",
    "    log_interval: int = 100 # how often progress should be printed\n",
    "    n_base_learners: int = 1 # how many base learners should be trained\n",
    "    print_mode_on: bool = True # if progress should be printed\n",
    "    store_data: bool = False # if models and plots should be stored\n",
    "    store_mdl_location: str = '/stored_models' # where saved models should be stored\n",
    "    store_fig_location: str = '/stored_models/figures/' # where saved figures should be stored\n",
    "    \n",
    "    # Below follows options specific for the ensemble_prep library\n",
    "    \n",
    "    directory: str = './data/' # directory contatining observations\n",
    "    highway_directory: str = 'final/' # specific directory containing relevant data\n",
    "    use_oversample: bool = False # if minority classes should be oversampled\n",
    "    downsample: bool = False # removes already oversampled minority class instances\n",
    "    scl_strategy: str = 'std' # input data scaling strategy. std or norm\n",
    "    keep: float = 1.0 # proportion of complete set to be kept\n",
    "    prediction_horizon: float = 3.5 # determines the cutoff of LC labels in the set (max is 5 s)\n",
    "    re_express_lanes: bool = True # convert neigboring lanes to 1/0 to indicate lane existance\n",
    "    split: float = 0.75 # percentage of how much data should go into training vs. testing\n",
    "\n",
    "options = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.device = torch.device(\"cuda\" if torch.cuda.is_available() and options.use_cuda else \"cpu\")\n",
    "print(options.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training history\n",
    "def plot_history(loss_history, acc_history, save_fig=False, fig_name=\"Foo\"):\n",
    "    if acc_history['train'][0] is not None:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "        ax1.plot(acc_history['train'],'tab:orange', lw=3, label='Train')\n",
    "        ax1.plot(acc_history['test'],'tab:blue', lw=3, label='Test')\n",
    "        ax1.set_xlabel(\"Epoch number\")\n",
    "        ax1.set_ylabel(\"Accuracy\")\n",
    "        ax1.set_title(f\"Training Accuracy vs Test Accuracy. Max test: {max(acc_history['test'])}\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(loss_history['train'],'tab:orange', lw=3)\n",
    "        ax2.plot(loss_history['test'],'tab:blue', lw=3)\n",
    "        ax2.set_xlabel(\"Epoch number\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Training loss vs Test loss\")\n",
    "        ax2.legend(['Training','Test'])\n",
    "        ax2.grid(True)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(16,6))    \n",
    "        ax.plot(loss_history['train'],'tab:orange', lw=3)\n",
    "        ax.plot(loss_history['test'],'tab:blue', lw=3)\n",
    "        ax.set_xlabel(\"Epoch number\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.set_title(\"Training loss vs Test loss\")\n",
    "        ax.legend(['Training','Test'])\n",
    "        ax.grid(True)\n",
    "    if save_fig:\n",
    "        plt.savefig(fig_name, format='pdf')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentionPredictionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.seq_len = 20\n",
    "        self.n_features = 40\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subset = self.data[idx]\n",
    "        trajectory = torch.Tensor(subset[0])\n",
    "        target = torch.from_numpy(subset[1]).to(torch.float32)\n",
    "        ttlc = torch.from_numpy(np.array(subset[2])).to(torch.float32)\n",
    "        return trajectory, target, ttlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, epoch, criterion, objective='regression', pre_process=None):\n",
    "    running_loss = batch_count = 0\n",
    "    accuracy = None\n",
    "    recall = np.zeros(3)\n",
    "    if objective == 'classification':\n",
    "        accuracy = 0.\n",
    "        total = correct = 0\n",
    "    \n",
    "    def print_progress(epoch, current_loss, iteration, ms_per_batch, accuracy=None, recall=[]):\n",
    "        def rd(value, order=2):\n",
    "            return round(value, order)\n",
    "        \n",
    "        base_msg = f'Training set ->> | Epoch: {epoch} | Iter: {iteration} |' \\\n",
    "                   f' ms/batch: {rd(ms_per_batch)} | Loss: {rd(current_loss)}|'\n",
    "        \n",
    "        if accuracy is None:\n",
    "            print(base_msg)\n",
    "        else:\n",
    "            cf_msg = f' Acc: {rd(accuracy)} |' \\\n",
    "                     f' Recall: [{rd(recall[0])} :: {rd(recall[1])} :: {rd(recall[2])}]|'\n",
    "            print(base_msg + cf_msg)\n",
    "            \n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target, _) in enumerate(data_loader):\n",
    "        data, target = data.to(options.device), target.to(options.device)\n",
    "        optimizer.zero_grad()\n",
    "        if pre_process is not None:\n",
    "            processed_data = pre_process(data)\n",
    "            output = model(processed_data, data)\n",
    "        else:\n",
    "            processed_data = None\n",
    "            output = model(data)\n",
    "        \n",
    "        if objective == 'regression':\n",
    "            loss = criterion(output, data[:, :, 4:])\n",
    "\n",
    "            if options.sparse_func == 'L1':\n",
    "                sp_loss = l1_loss(model, data)\n",
    "            elif options.sparse_func == 'KL':\n",
    "                sp_loss = kl_loss(model, data)\n",
    "            else:\n",
    "                sp_loss = 0\n",
    "            loss += options.reg_param * sp_loss\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), options.clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if objective == 'classification':\n",
    "            _, y_pred = torch.max(output.data, 1)\n",
    "            y_true = torch.max(target, 1)[1]\n",
    "            total += target.size(0)\n",
    "            correct += (y_pred == y_true).sum().item()\n",
    "            accuracy = 100. * correct / total\n",
    "            for k in range(len(recall)):\n",
    "                recall[k] += recall_score(y_true.cpu(), y_pred.cpu(), labels=[k], average='micro', zero_division=0)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        del data, processed_data, target\n",
    "        \n",
    "        if batch_idx % options.log_interval == 0 and batch_idx > 0:\n",
    "            current_loss = running_loss / (batch_idx + 1)\n",
    "            iteration = batch_idx + 1 + epoch * len(data_loader)\n",
    "            ms_per_batch = (time.time() - start_time)*1000. / options.log_interval\n",
    "            \n",
    "            if options.print_mode_on:\n",
    "                print_progress(epoch, current_loss, iteration, ms_per_batch, accuracy, recall/batch_count)\n",
    "            \n",
    "            start_time = time.time()\n",
    "    return running_loss / (batch_idx + 1), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, criterion, objective='regression', pre_process=None):\n",
    "    running_loss = batch_count = 0\n",
    "    accuracy = None\n",
    "    recall = np.zeros(3)\n",
    "    if objective == 'classification':\n",
    "        accuracy = 0.\n",
    "        total = correct = 0\n",
    "    \n",
    "    def print_progress(test_loss, accuracy=None, recall=[]):\n",
    "        def rd(value, order=2):\n",
    "            return round(value, order)\n",
    "        \n",
    "        base_msg = f'Test set ->> | loss: {rd(test_loss)} |'\n",
    "        if accuracy is None:\n",
    "            print(base_msg)\n",
    "        else:\n",
    "            cf_msg = f' Acc: {rd(accuracy)} |' \\\n",
    "                     f' Recall: [{rd(recall[0])} :: {rd(recall[1])} :: {rd(recall[2])}]|'\n",
    "            print(base_msg + cf_msg)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target, _ in data_loader:\n",
    "            data, target = data.to(options.device), target.to(options.device)\n",
    "            if pre_process is not None:\n",
    "                processed_data = pre_process(data)\n",
    "                output = model(processed_data, data)\n",
    "            else:\n",
    "                processed_data = None\n",
    "                output = model(data)\n",
    "\n",
    "            if objective == 'classification':\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                _, y_pred = torch.max(output.data, 1)\n",
    "                y_true = torch.max(target, 1)[1]\n",
    "                total += target.size(0)\n",
    "                correct += (y_pred == y_true).sum().item()\n",
    "                accuracy = 100. * correct / total\n",
    "                for k in range(len(recall)):\n",
    "                    recall[k] += recall_score(y_true.cpu(), y_pred.cpu(), labels=[k], average='micro', zero_division=0)\n",
    "            else:\n",
    "                loss = criterion(output, data[:, :, 4:])\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            del data, processed_data, target\n",
    "    \n",
    "    test_loss = running_loss / (batch_count)\n",
    "    if options.print_mode_on:\n",
    "        print_progress(test_loss, accuracy, recall/batch_count)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(pre_process, train_loader, name, cat_feature_encoder=None):\n",
    "    gen = iter(train_loader)\n",
    "    data, target, _ = next(gen)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if feature_encoder is not None:\n",
    "            categorical, _ = cat_feature_encoder(data.to(options.device))\n",
    "        else:\n",
    "            categorical = None\n",
    "        X = pre_process(data.to(options.device))\n",
    "    y_train = torch.max(target, 1)[1].cpu().detach().numpy()\n",
    "    \n",
    "    if categorical is not None:\n",
    "        X = torch.cat((X, categorical), dim=1)\n",
    "        \n",
    "    X_train = X.cpu().detach().numpy()\n",
    "    \n",
    "    svc=SVC(probability=True, kernel='linear')\n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    svc_name = options.store_mdl_location + \"SVMClassifier-\" + name + \".sav\"\n",
    "\n",
    "    if options.store_data:\n",
    "        pickle.dump(svc, open(svc_name, 'wb'))\n",
    "        \n",
    "#     gbc = HistGradientBoostingClassifier().fit(X_train, y_train)\n",
    "#     gbc_name = \"./models/GBC\" + name + \".sav\"\n",
    "    \n",
    "#     if options.store_data:\n",
    "#         pickle.dump(gbc, open(gbc_name, 'wb'))\n",
    "    \n",
    "    if options.print_mode_on:\n",
    "        print('Sci-kit done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(autoencoder, values):\n",
    "    loss = 0\n",
    "    encoder, decoder = list(autoencoder.children())\n",
    "    encoder = list(encoder.children())\n",
    "    \n",
    "    _, (hidden, _) = encoder[0](values)\n",
    "    values = hidden[-1,:,:]\n",
    "    values = encoder[1](values)\n",
    "    values = F.relu(values)\n",
    "\n",
    "    loss += torch.sum(values.abs(), dim=1).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(autoencoder, values):\n",
    "    loss = 0\n",
    "    RHO = 5e-2\n",
    "    encoder, _ = list(autoencoder.children())\n",
    "    encoder = list(encoder.children())\n",
    "    \n",
    "    _, (hidden, _) = encoder[0](values)\n",
    "    values = hidden[-1,:,:]\n",
    "    values = encoder[1](values)\n",
    "    values = F.relu(values)\n",
    "    loss += kl_divergence(RHO, values)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(rho, rho_hat):\n",
    "    rho_hat = torch.mean(torch.sigmoid(rho_hat), 1) # sigmoid because we need the probability distributions\n",
    "    rho = torch.tensor([rho] * len(rho_hat)).to(device)\n",
    "    return torch.sum(rho * torch.log(rho/rho_hat) + (1 - rho) * torch.log((1 - rho)/(1 - rho_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, name, criterion, train_loader, test_loader, objective='regression',\n",
    "         learning_rate=options.lr, pre_process=None, schedule=False):\n",
    "    n_bad_epochs = best_epoch = best_acc = 0\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    loss_history = {'train':[], 'test':[]}\n",
    "    acc_history = {'train':[], 'test':[]}\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if schedule:\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    \n",
    "    model_name = options.store_mdl_location + name + \".pth\"\n",
    "    figure_name = options.store_fig_location + name + \".pdf\"\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(1, options.epochs+1):\n",
    "            train_loss, train_acc = train(model, train_loader, optimizer, epoch, criterion,\n",
    "                                          objective=objective, pre_process=pre_process)\n",
    "            test_loss, test_acc = test(model, test_loader, criterion,\n",
    "                                       objective=objective, pre_process=pre_process)\n",
    "            \n",
    "            # Store intermediate results\n",
    "            acc_history['train'].append(train_acc)\n",
    "            acc_history['test'].append(test_acc)\n",
    "            \n",
    "            loss_history['train'].append(train_loss)\n",
    "            loss_history['test'].append(test_loss)\n",
    "            \n",
    "            if test_loss < best_loss:\n",
    "                n_bad_epochs = 0\n",
    "                best_loss = test_loss\n",
    "                if objective == 'regression':\n",
    "                    best_epoch = epoch\n",
    "                    if options.store_data:\n",
    "                        torch.save(model.state_dict(), model_name)\n",
    "            else:\n",
    "                if objective == 'regression':\n",
    "                    n_bad_epochs += 1\n",
    "            \n",
    "            if objective == 'classification':\n",
    "                if test_acc > best_acc:\n",
    "                    n_bad_epochs = 0\n",
    "                    best_acc = test_acc\n",
    "                    best_epoch = epoch\n",
    "                    if options.store_data:\n",
    "                        torch.save(model.state_dict(), model_name)  \n",
    "                else:\n",
    "                    n_bad_epochs += 1\n",
    "                \n",
    "            if n_bad_epochs >= options.epoch_limit:\n",
    "                print(f'Number of consecutive bad epochs exceeded ({options.epoch_limit}). Employing early stopping...')\n",
    "                break\n",
    "                \n",
    "            if epoch % 10 == 0:\n",
    "                if objective == 'classification':\n",
    "                    print('\\n Historically best test accuracy: \\x1b[31m{:5.2f}% \\x1b[0m on epoch: {}\\n'.format(best_acc, best_epoch))\n",
    "                else:\n",
    "                    print('\\n Historically best test loss: \\x1b[31m{:5.2f}% \\x1b[0m on epoch: {}\\n'.format(best_loss, best_epoch))\n",
    "            \n",
    "            if schedule:\n",
    "                scheduler.step()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted early...\")\n",
    "    finally:\n",
    "        del criterion, optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print('Finished Training! \\n')\n",
    "    \n",
    "    plot_history(loss_history, acc_history, save_fig=options.store_data, fig_name=figure_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_prep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = EnsemblePrep(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_training(network_class, input_data, objective='classification'):\n",
    "    torch.manual_seed(input_data.seed)\n",
    "    input_data.init_scrambler()\n",
    "    print(f'Training network with {options.n_base_learners} ensemble(s)')\n",
    "    for n in range(1, options.n_base_learners + 1):\n",
    "        train_data, test_data = input_data.get_train_val()\n",
    "        model = network_class().to(options.device)\n",
    "        name = type(model).__name__ + \"-\" + str(n)\n",
    "        \n",
    "        train_loader = DataLoader(IntentionPredictionDataset(train_data), batch_size=options.batch_size,\n",
    "                                  shuffle=True,  drop_last=True)\n",
    "        test_loader = DataLoader(IntentionPredictionDataset(test_data), batch_size=len(test_data),\n",
    "                                  shuffle=True,  drop_last=True)\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        model = main(model, name, criterion, train_loader, test_loader, objective)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./mdl-implementation/VanillaPredictor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_training(VanillaCNN, in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./mdl-implementation/AutoEncoder.ipynb\n",
    "%run ./mdl-implementation/AEClassifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_ensemble_training(auto_encoder, ae_classifier, input_data, SVM=False, use_cat=False):\n",
    "    torch.manual_seed(input_data.seed)\n",
    "    input_data.init_scrambler()\n",
    "    embedding_size = 128\n",
    "    \n",
    "    if use_cat:\n",
    "        feat_encode = CategoricalFeatureEncoder().to(device)\n",
    "        feat_encode.eval()\n",
    "    \n",
    "    print(f'Training {options.n_base_learners} base learner(s)')\n",
    "    for n in range(1, options.n_base_learners + 1):\n",
    "        train_data, test_data = input_data.get_train_val()\n",
    "        ae_model = auto_encoder(embedding_size=embedding_size).to(options.device)\n",
    "        name = type(ae_model).__name__ + \"-\" + str(n) + \"-\" + str(embedding_size)\n",
    "        \n",
    "        train_loader = DataLoader(IntentionPredictionDataset(train_data), batch_size=options.batch_size,\n",
    "                                  shuffle=True,  drop_last=True)\n",
    "        test_loader = DataLoader(IntentionPredictionDataset(test_data), batch_size=len(test_data),\n",
    "                                  shuffle=True,  drop_last=True)\n",
    "        \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        ae_model = main(ae_model, name, criterion, train_loader, test_loader,\n",
    "                        learning_rate=options.ae_lr, objective='regression')\n",
    "        \n",
    "        \n",
    "        for p in ae_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        cf_model = AEClassifier(n_inputs=embedding_size).to(options.device)\n",
    "        name = type(cf_model).__name__ + \"-\" + str(n) + \"-\" + str(embedding_size)\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        cf_model = main(cf_model, name, criterion, train_loader, test_loader,\n",
    "                        objective='classification', pre_process=ae_model.encoder, schedule=True)\n",
    "        \n",
    "        if SVM:\n",
    "            train_loader = DataLoader(IntentionPredictionDataset(train_data), \n",
    "                                      batch_size=len(train_data), shuffle=True,  drop_last=True)\n",
    "            add_name = str(n) + \"-\" + str(embedding_size)\n",
    "            if use_cat:\n",
    "                train_svm(ae_model.encoder, train_loader, add_name, feat_encode)\n",
    "            else:\n",
    "                train_svm(ae_model.encoder, train_loader, add_name)\n",
    "                \n",
    "        \n",
    "        del ae_model, cf_model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ae_ensemble_training(AutoEncoderDecoder, AEClassifier, in_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
